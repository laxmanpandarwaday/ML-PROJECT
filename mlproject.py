# -*- coding: utf-8 -*-
"""MLPROJECT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eqHtr9fGZ6EVEM-0VbZjdEWhpejGF5l2

# **IMPORTING LIBRARIES AND DATASET**
"""

# Commented out IPython magic to ensure Python compatibility.
#IMPORTING LIBRARIES
import pandas as pd
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
from sklearn import preprocessing

df = pd.read_excel("Bank_Personal_Loan_Modelling.xlsx")

df.head()

# RENAMING THE COLUMNS SO AS TO REMOVE THE WHITESPACES PRESENT BETWEEN SOME OF THEM
df.columns = ["ID","Age","Experience","Income","ZIPCode","Family","CCAvg","Education","Mortgage","PersonalLoan","SecuritiesAccount","CDAccount","Online","CreditCard"]
df.head()

# SIZE OF THE DATASET (ROW,COLUMNS)
df.shape

# THERE ARE NO NULL VALUES PRESENT IN THE DATASET
df.isnull().values.any()

df[['Family','Education','PersonalLoan','SecuritiesAccount','CDAccount','Online','CreditCard']]=df[['Family','Education','PersonalLoan','SecuritiesAccount','CDAccount','Online','CreditCard']].astype('int')

df.dtypes

df.describe()

"""# **CLEANING THE DATASET**"""

# AS WE CAN SEE ABOVE THAT THE MINIMUM EXPERIENCE LEVEL IS -3, WHICH SHOULD BE REMOVED AS EXPERIENCE CANNOT BE NEGATIVE
neg = df.Experience < 0
neg.value_counts()

# THERE ARE 52 NEGATIVE VALUES, WHICH IS AN ERROR
# THEREFORE REPLACING THESE VALUES BY ABSOLUTE VALUE
df['Experience'] = abs(df['Experience'])

df['Experience'].describe()

"""# **EXPLORATORY DATA ANALYSIS(EDA)**"""

# NUMBER OF UNIQUES IN EACH COLUMN
df.nunique()

# DROPPING THE ID, ZIPCode COLUMNs AS THEY DOEN'T CONTRIBUTE ANYTHING TO OUR DATA ANALYSIS
df.drop(['ID'],inplace=True,axis=1)
df.drop(['ZIPCode'],inplace=True,axis=1)
df

# NUMBER OF CUSTOMERS WITH 0 MORTGAGE
df['Mortgage'].isin([0]).sum()

# NUMBER OF CUSTOMERS WITH ZERO CREDIT CARD SPENDINGS PER MONTH
df['CCAvg'].isin([0]).sum()

# Value counts of all categorical columns.
print(df.Online.value_counts())
print('\n')
print(df.PersonalLoan.value_counts())
print('\n')
print(df.SecuritiesAccount.value_counts())
print('\n')
print(df.CDAccount.value_counts())
print('\n')
print(df.CreditCard.value_counts())
print('\n')
print(df.Family.value_counts())
print('\n')
print(df.Education.value_counts())

# UNIVARIATE AND BIVARIATE ANALYSIS.
# THIS SHOWS THE CORELATION BETWEEN EACH AND EVERY COLUMN PAIR OF THE DATASET, SOME OF THE ARE USEFUL.
# EXAMPLE INCOME AND EDUCATION HAVE A LINEAR RELATIONSHIP.
sns.pairplot(df.iloc[:,1:])

# THIS OVERLAPS PERSONAL LOAN GRAPHS, SHOWS THE INTERSECTION OF PERSONAL LOAN WITH OTHER COLUMNS
sns.pairplot(df.iloc[:,1:],hue ='PersonalLoan')

# Income and CCAvg is moderately correlated
# Age and Experience is highly correlated
# Also there is a moderate correlation b/w Income and Loan
corr=df.corr()
plt.subplots(figsize =(15, 10))
sns.heatmap(corr,annot=True)

# SINCE AGE AND EXPERIENCE ARE HIGHLY CORRELATED, THEREFORE CONSIDERING ONLY ONE OF THEM IN OUR PREDICTIONS
df.drop(['Experience'],inplace=True,axis=1)
df

"""# **UNIVARIATE ANALYSIS**"""

sns.distplot(df.Age)

sns.distplot(df.Age)

sns.distplot(df.CCAvg)

sns.distplot(df.Mortgage)

"""# **BIVARIATE AND MULTIVARIATE ANALYSIS**"""

# SHOW COUNT OF PEOPLE WHO HAVE TAKEN A LOAN and WHO HAVEN'T
sns.countplot(x='PersonalLoan',data=df)

sns.boxplot(x='PersonalLoan',y='Income',data=df)

# NO SPECIFIC CORRELATION BETWEEN AGE AND PERSONAL LOAN, AS ALL AGE GROUPS HAVE PEOPLE WHO HAVE TAKEN LOAN AND WHO HAVE NOT TAKEN LOAN.
# PEOPLE WHO HAVE TAKEN LOANS BELONG TO A PARTICULAR INCOME LEVEL.
sns.boxplot(x='Education',y='Income',hue='PersonalLoan',data=df)

# CUSTOMERS WITH AND WITHOUT PERSONAL LOANS, BOTH HAVE HIGH MORTGAGES
# THEREFORE, MORTGAGE IS NOT A VERY GOOD VARIABLE FOR PREDICTIONS.
sns.boxplot(x="Education", y='Mortgage', hue="PersonalLoan",data = df)

# MAJORITY OF CUSTOMERS, WHO TAKE LOAN, DO NOT HAVE A SECURITY ACCOUNT
sns.countplot(x='SecuritiesAccount',hue='PersonalLoan',data=df)

# MAJORITY OF CUSTOMERS WITH EDUCATION LEVEL 1 DON'T TAKE LOANS
sns.countplot(x='Education',hue='PersonalLoan',data=df)

# CUSTOMERS WITH NO CREDIT CARDS ARE LESS LIKELY TO TAKE LOANS.
sns.countplot(x='CreditCard',hue='PersonalLoan',data=df)

#FAMILIES WITH HIGHER INCOME ARE MORE LIKELY TO TAKE LOANS.
sns.boxplot(x='Family',y='Income',hue='PersonalLoan',data=df)

# CUSTOMERS WITH A CDACCOUNT TEND TO TAKE LOAN.
sns.countplot(x='CDAccount',hue='PersonalLoan',data=df)

# With the increase in income, ccavg also increases, and people tend to take more loans.
sns.scatterplot(x='CCAvg',y='Income',hue = 'PersonalLoan',data = df)

# NO CORRELATION BETWEEN A CUSTOMER USING INTERNET BANKING FACILITIES AND TAKING A PERSONAL LOAN.
sns.countplot(x='Online',hue='PersonalLoan',data=df)

sns.boxplot(x='PersonalLoan',y='CCAvg',data=df)

"""# **NECESSARY TRANSFORMATIONS FOR FEATURE VARIABLES**"""

y=df['PersonalLoan']
x=df.drop(['PersonalLoan'],axis=1)

from sklearn.preprocessing import PowerTransformer
pt = PowerTransformer(method = "yeo-johnson", standardize = False)
pt.fit(x['Income'].values.reshape(-1,1))
x['Income'] = pt.transform(x['Income'].values.reshape(-1,1))
sns.distplot(x.Income)

pt = PowerTransformer(method = "yeo-johnson", standardize = False)
pt.fit(x['CCAvg'].values.reshape(-1,1))
x['CCAvg'] = pt.transform(x['CCAvg'].values.reshape(-1,1))
sns.distplot(x.CCAvg)

x['Mortgage_Int'] = pd.cut(x['Mortgage'],
bins = [0,100,200,300,400,500,600,700],
labels = [0,1,2,3,4,5,6],
include_lowest = True)
x.drop('Mortgage',axis = 1, inplace = True)
sns.distplot(x.Mortgage_Int)

"""# **STANDARDIZING AND SPLITTING DATA**"""

def standardization(X_train,X_test):
  scaler=preprocessing.StandardScaler()
  X_train=scaler.fit_transform(X_train)
  X_test=scaler.transform(X_test)
  return X_train,X_test

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.30,stratify = y ,random_state=42)
x_train,x_test = standardization(x_train,x_test)

"""# **PREDICTION MODELS**

## **LOGISTIC REGRESSION MODEL**
"""

from sklearn.linear_model import LogisticRegression
lrmodel = LogisticRegression()
lrmodel.fit(x_train,y_train)
y_pred = lrmodel.predict(x_test)

from sklearn.metrics import accuracy_score
score = accuracy_score(y_train,lrmodel.predict(x_train), normalize=True)
print(" TRAINING Accuracy :",int(score*100),end='%')

from sklearn.metrics import accuracy_score
score = accuracy_score(y_test, y_pred, normalize=True)
print(" TESTING Accuracy :",int(score*100),end='%')

from sklearn.metrics import confusion_matrix
confusion_matrix(y_test,y_pred)

from sklearn import metrics
print("The Classification report is")
print(metrics.classification_report(y_test, y_pred))
roc=metrics.roc_auc_score(y_test, y_pred)
print("ROC value for logistic model is "+ str(roc*100) + "%")

"""## **NAIVE BAYES**"""

from sklearn.naive_bayes import GaussianNB
nbmodel = GaussianNB()
nbmodel.fit(x_train,y_train)
y_pred = nbmodel.predict(x_test)

score = nbmodel.score(x_train,y_train)
print(" TRAINING Accuracy :",int(score*100),end='%')

score = nbmodel.score(x_test,y_test)
print(" TESTING Accuracy :",int(score*100),end='%')

confusion_matrix(y_test,y_pred)

print("The Classification report is")
print(metrics.classification_report(y_test, y_pred))
roc=metrics.roc_auc_score(y_test, y_pred)
print("ROC value for naive bayes model is "+ str(roc*100) + "%")

"""## **KNN MODEL**"""

from sklearn.neighbors import KNeighborsClassifier
knnmodel = KNeighborsClassifier()
knnmodel.fit(x_train,y_train)
y_pred = knnmodel.predict(x_test)

score = knnmodel.score(x_train,y_train)
print(" TRAINING Accuracy :",int(score*100),end='%')

score = knnmodel.score(x_test,y_test)
print(" TESTING Accuracy :",int(score*100),end='%')

confusion_matrix(y_test,y_pred)

print("The Classification report is")
print(metrics.classification_report(y_test, y_pred))
roc=metrics.roc_auc_score(y_test, y_pred)
print("ROC value for knn model is "+ str(roc*100) + "%")

"""## **DECISION TREE**"""

from sklearn.tree import DecisionTreeClassifier
dtmodel = DecisionTreeClassifier(max_depth = 5, random_state=1)
dtmodel.fit(x_train,y_train)
y_pred = dtmodel.predict(x_test)

score = dtmodel.score(x_train,y_train)
print(" TRAINING Accuracy :",int(score*100),end='%')

score = dtmodel.score(x_test,y_test)
print(" TESTING Accuracy :",int(score*100),end='%')

confusion_matrix(y_test,y_pred)

print("The Classification report is")
print(metrics.classification_report(y_test, y_pred))
roc=metrics.roc_auc_score(y_test, y_pred)
print("ROC value for decision tree model is "+ str(roc*100) + "%")

"""# **RANDOM FOREST CLASSIFIER MODEL**"""

from sklearn.ensemble import RandomForestClassifier
rfcmodel = RandomForestClassifier(max_depth=5, random_state=42)
rfcmodel.fit(x_train,y_train)
rfcmodel.predict(x_test)

score = rfcmodel.score(x_train,y_train)
print(" TRAINING Accuracy :",int(score*100),end='%')

score = rfcmodel.score(x_test,y_test)
print(" TESTING Accuracy :",int(score*100),end='%')

confusion_matrix(y_test,y_pred)

print("The Classification report is")
print(metrics.classification_report(y_test, y_pred))
roc=metrics.roc_auc_score(y_test, y_pred)
print("ROC value for random forest classifier model is "+ str(roc*100) + "%")

"""# **GRADIENT BOOSTING CLASSIFIER MODEL**"""

from sklearn.ensemble import GradientBoostingClassifier
gbmodel = GradientBoostingClassifier(random_state=42)
gbmodel.fit(x_train,y_train)
y_pred = gbmodel.predict(x_test)

score = gbmodel.score(x_train,y_train)
print(" TRAINING Accuracy :",int(score*100),end='%')

score = gbmodel.score(x_test,y_test)
print(" TESTING Accuracy :",int(score*100),end='%')

confusion_matrix(y_test,y_pred)

print("The Classification report is")
print(metrics.classification_report(y_test, y_pred))
roc=metrics.roc_auc_score(y_test, y_pred)
print("ROC value for gradient boosting classifier model is "+ str(roc*100) + "%")

"""# **COMPARISION BETWEEN DIFFERENT MODELS**"""

sensitivity = [52,63,69,95,95,94]
train_error = [10,4,5,2,1,1]
test_error = [10,4,5,2,2,2]
ROC = [73,81,82,96,96,96]
col = {'Train_error' : train_error, 'Test_error' : test_error, 'Sensitivity' : sensitivity}
models = ['Naive Bayes','KNN','Logistic Regression','Decision Tree','Random Forest','Gradient Boosting']
data = pd.DataFrame(data = col, index = models)
data.plot(kind='bar')

"""Decision Tree, Random Forest and Gradient Boosting Classifier are close in terms of Accuracy, but the best predictions here, are made by Decision Tree model.
## **Therefore, Decision Tree Model is best for this data !**
# **BUSINESS UNDERSATNDING**
**Here are some key takeaways from this model :**

 *   Income plays a major role in predicting whether a customer will take loan or not. Usually people with Income
between 120K - 170K dollars, tend to take loans.

*   Customers with education level 2 and 3 are more likely to take loans as compared to the customers with education level 1.

*   Customers with credit cards are also more likely to take a loan.

*   Customers with certificate of deposit accounts are more likely to take a loan.

*   Customers without securites account, are more likely to take a loan.

*   Customers with average credit card spending between 3k - 5k dollars per month are more likely to take a
loan.

*   The Decision Tree model predicts 137 customers out of 144 who have actually taken the loan. Therefore this
will help us to predict customers who are willing to take a loan from the bank. Also, the Random Forest
Classifier Model and Gradient Boosting Classifier Model work good enough !
"""